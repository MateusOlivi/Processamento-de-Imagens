 

 

 

  

{EEE TRAEACTIONE CH IDEMREDOE ANG DATA BNONCERING, OL 2, ADL, SAIUARHOFEEROAMY 2000

Natural Language Grammatical Inference
with Recurrent Neural Networks

Steve Lawrence, Member, (ZEB, C, ec Giles, Fellow, IEEE, and Sandiway Fong

 
 

Abstract. paper examines the duce eranos of «compos grammar wah nuda oatworke epoca, e tsk conser
{ott of irsnng 3 netvorkt lacxTy eure engage sentences ob rats Or ungyarenacal. oreby exiting te tang Rt
* at sctimemory power plovded by he Proeple and Peramatersingua uameworh, of Govomment ences trey Noureb
‘pedo ar rained wri te cslon 4 tne ws, wate cormponenta assumed dy Chomaty, nor oem o produce ihe sare
[betynents as nein spaakarn on enariy ganmaticasgrarmetea! dara. How a recent neural network cid postess Segal
‘apabity ed he propor of varius comma raced nea neon erent br creed. Th jcktam eNO aN
betanvor wtih a else nol prosor wil emer rarrbats and vaining was ietaty dict. However afer mpleenivg ceva!

leclquea aed ingrown ab convergatos cf te gradient deacen backgropegaion trougeie raking aipotin gvicans
fearing we possiia, twos kare at Orin octecures a berate 0 learn a appcopdaa graeay. The operon of
‘tes na ining analyte. bay, fe exrmcdon of Roane fer ol determhseSc ra ciate aero a nwesicsied,

lnses Terrs—Recront newel networks, ara language rocketing, gare) aver, gavemmenk and sng her,
(fodent doncerd, emafsted anneatng, pechloeand-palwnetar ramework, sara mEAZH,
+

 

 

1 {srnoouction
TPs paper conser the ask of cnyng natural aon notwors autor theaptaton of he netvorks

Language senlences at grammalkxal oc ugrammatical, and the taining algorithy and favestigntes rule extiaction,
We attempt i train neural networks, without the bifucca- This paper is organized aa follows: Section 2 provides the

 

Hon Inte Ietrned vm Innate components asnvmed. by _sacivaton forte tank, tternpted Section 3 pruvidea 9
Chomeky, te produce the same judgments ax native Introduction to formal grammars and grammatical inlce-
spetkers on sharply glamcatie!/ungrammaties! data, ence and deserbos the dala, Sectina 4 ists the secure!
Only recarrent newiral networks are investigated for neural network models investignted and provides details ol
computational essen. Computationaly, eeurreet acura! the deta encoding fr the newosks. Sexton & presents the
networks ant moe powerful than feutforwand networks geplis of Inustigation ino vatkaus training brrisics and
and gone tocurneit architsetutcs have been shown te be at investigation of talning with simufaled annealing. Section 6
feast Turing equivalent (53), 151). We invenigate the presents the main results and simulation dstals and
propertios of various popular recurrent neurat network iqvestigates the operation of the netiwarks. The extraction
architectures, in particufot Elman, Narendra and Parthasar~ of rites le the form of deterministic finite state automata is
athy (N@P), and Willlans acd Zipser W&Z) recarrent investigated In Section 7 and Section B presenta.a discustion
networks, and ako Frasconl-GoriSoda (PGS) tocally recur of the results and conclusions,
rent network, We find that both Elman are WAZ rcursent
‘earal neticorks are able wo Team an appropriate grammar
Bier Implementing techniques for ueproving the conver- 2 MOTIVATION
genes of the gradient descent hese backpropagation 2.t  Representatlona Powar
Ihraugh- (ine toning algorithm, We analyze the operation Natural Language has Uditonally been buneled axing
of tbe networks and investigate a rule opprosination ef aymbotic enmpbtation and meuraive processes. The most
itt the recurrent network fs lamned—apeciiealy, ARE ecenful chaste language tools ave hewn based on
tatraction of rues i the form of doterminsic Fille sate Brreseane deceptions tock ts apart or hidden Maskov
sutomata 3
amudels. However, Gnitestate models cannot feproseat
Previous work [38 has compared neural otworks with Hveehiealstructores ae Foard mt natal Lingosie? Fa
other machioe lncring paradigms mt thiy probleorahin_ fn the past few yeas, several recurrent meurd network
Fares thy Pe Schilecurs hav emerged which have Been nord for
work fortes on recuprent neural networks, investigates Somunatkal intron ISP [EL LOL, lsh, Recurrent
Feuol neworks have been used for several srstiernatueal 4
fanguage problems, eg. papert usiag the Elraan noterark
for natural language tasks inehude: [1D (221, (242, 158}. 9) +

   

 

 

5 The autre ae wil NOG Rew Pte, # Audgeodence Way.

 

 
   

Pri, OCS v
an: Hase ss suulian voi econ, [Neural nobwark mandels bave born shown ta be able bo

our mele H Mer 16: med 9 Sop 1937; wate 8 be ,
29. 1c The seca cei alpen be an et of Bde
‘Mare del bended be nc fort arora. Ta

 

Far Uiooauon on eating rpc of i ati, pave bau oma bc
Redeprarg 0 mfr IEECS Lag Dna TOES, “Agvet lacorroy oy proce for ely sad pravoam Fs.

rowaemeaman enact

  

  

  
 
   

  
 
 

 
  
  
 
